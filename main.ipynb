{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Multiclass Logistic Regression on the Iris Dataset\n",
    "# \n",
    "# **Objective:** Implement a multiclass logistic regression classifier on the Iris dataset.\n",
    "# \n",
    "# **Steps:**\n",
    "# 1. Load and split the dataset.\n",
    "# 2. Preprocess using three approaches:\n",
    "#    - Unprocessed\n",
    "#    - Normalized (Min-Max Scaling)\n",
    "#    - Standardized (Z-score Normalization)\n",
    "# 3. Define the softmax function and cross-entropy loss.\n",
    "# 4. Implement training with both gradient descent (GD) and stochastic gradient descent (SGD).\n",
    "# 5. Evaluate and visualize the training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns  # For a prettier confusion matrix plot (optional)\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 2. Load the Iris Dataset\n",
    "# \n",
    "# We load the dataset using `sklearn.datasets.load_iris()`, extract the four features and class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data    # shape: (150, 4)\n",
    "y = iris.target  # shape: (150,)\n",
    "\n",
    "# Check the shapes and classes\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "print(\"Unique classes:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 3. Preprocess the Data\n",
    "# \n",
    "# We first split the data into training (60%) and testing (40%) sets. Then we define two preprocessing functions:\n",
    "# \n",
    "# - **Normalization (Min-Max Scaling):** Rescales features to the range [0, 1].  \n",
    "# - **Standardization (Z-score):** Transforms features to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset (60% training, 40% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"Min-Max Normalization to [0,1]\"\"\"\n",
    "    return (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\"Z-score Standardization\"\"\"\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Create preprocessed versions of the data\n",
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm  = normalize(X_test)\n",
    "\n",
    "X_train_std = standardize(X_train)\n",
    "X_test_std  = standardize(X_test)\n",
    "\n",
    "# You can also train on the unprocessed data (X_train and X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions\n",
    "# \n",
    "# We now define the following:\n",
    "# \n",
    "# - **Softmax function:** Computes probabilities from logits.\n",
    "# - **One-hot encoding:** Converts integer class labels into one-hot vectors.\n",
    "# - **Categorical cross-entropy loss:** Computes the loss over m samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each row of the input z.\n",
    "    \"\"\"\n",
    "    # subtract max for numerical stability\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    \"\"\"\n",
    "    Convert label vector y to one-hot encoded matrix.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    y_encoded = np.zeros((m, num_classes))\n",
    "    y_encoded[np.arange(m), y] = 1\n",
    "    return y_encoded\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the categorical cross-entropy loss.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    # Add a small value to avoid log(0)\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 5. Implement the Multiclass Logistic Regression Model\n",
    "# \n",
    "# We create a class `LogisticRegressionMulti` that implements:\n",
    "# \n",
    "# - **Batch Gradient Descent (GD):** The weight update is performed using all samples.\n",
    "# - **Stochastic Gradient Descent (SGD):** The weight update is performed on one sample at a time.\n",
    "# \n",
    "# Both methods track the training loss at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMulti:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit_gd(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Batch Gradient Descent.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.W = np.zeros((n, num_classes))\n",
    "        y_encoded = one_hot_encode(y, num_classes)\n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            logits = np.dot(X, self.W)\n",
    "            probs = softmax(logits)\n",
    "            loss = compute_loss(y_encoded, probs)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Gradient calculation\n",
    "            grad = np.dot(X.T, (probs - y_encoded)) / m\n",
    "            self.W -= self.learning_rate * grad\n",
    "            \n",
    "            if self.verbose and epoch % 100 == 0:\n",
    "                print(f\"[GD] Epoch {epoch}/{self.epochs} - Loss: {loss:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def fit_sgd(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Stochastic Gradient Descent.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.W = np.zeros((n, num_classes))\n",
    "        y_encoded = one_hot_encode(y, num_classes)\n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            for i in indices:\n",
    "                xi = X[i:i+1]  # shape (1, n)\n",
    "                yi = y_encoded[i:i+1]  # shape (1, num_classes)\n",
    "                logits = np.dot(xi, self.W)\n",
    "                probs = softmax(logits)\n",
    "                grad = np.dot(xi.T, (probs - yi))\n",
    "                self.W -= self.learning_rate * grad\n",
    "            # Compute loss at the end of epoch\n",
    "            logits = np.dot(X, self.W)\n",
    "            probs = softmax(logits)\n",
    "            loss = compute_loss(y_encoded, probs)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if self.verbose and epoch % 100 == 0:\n",
    "                print(f\"[SGD] Epoch {epoch}/{self.epochs} - Loss: {loss:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logits = np.dot(X, self.W)\n",
    "        probs = softmax(logits)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 6. Train and Evaluate the Model\n",
    "# \n",
    "# We now run experiments using both GD and SGD. In each case, we demonstrate training on (for example) normalized data.\n",
    "# \n",
    "# You can repeat the following steps with:\n",
    "# - The unprocessed data: `X_train, X_test`\n",
    "# - The standardized data: `X_train_std, X_test_std`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### 6.1 Training with Batch Gradient Descent (GD) on Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model using GD on normalized data\n",
    "model_gd = LogisticRegressionMulti(learning_rate=0.1, epochs=1000, verbose=True)\n",
    "model_gd.fit_gd(X_train_norm, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_gd = model_gd.predict(X_test_norm)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_gd = accuracy_score(y_test, y_pred_gd)\n",
    "print(\"\\n[GD] Test Accuracy: {:.2f}%\".format(accuracy_gd * 100))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_gd))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### 6.2 Visualize the Loss Curve for GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model_gd.loss_history, label=\"GD Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss Curve (Gradient Descent)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.3 Training with Stochastic Gradient Descent (SGD) on Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the model using SGD on normalized data\n",
    "model_sgd = LogisticRegressionMulti(learning_rate=0.01, epochs=1000, verbose=True)\n",
    "model_sgd.fit_sgd(X_train_norm, y_train)\n",
    "\n",
    "# Make predictions on test set\n",
    "y_pred_sgd = model_sgd.predict(X_test_norm)\n",
    "\n",
    "# Evaluate the performance\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "print(\"\\n[SGD] Test Accuracy: {:.2f}%\".format(accuracy_sgd * 100))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_sgd))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### 6.4 Visualize the Loss Curve for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model_sgd.loss_history, label=\"SGD Loss\", color='orange')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss Curve (Stochastic Gradient Descent)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
