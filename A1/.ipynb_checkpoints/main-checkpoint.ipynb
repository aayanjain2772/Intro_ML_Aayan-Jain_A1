{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Multiclass Logistic Regression on the Iris Dataset\n",
    "# \n",
    "# **Objective:** Implement a multiclass logistic regression classifier on the Iris dataset.\n",
    "# \n",
    "# **Steps:**\n",
    "# 1. Load and split the dataset.\n",
    "# 2. Preprocess using three approaches:\n",
    "#    - Unprocessed\n",
    "#    - Normalized (Min-Max Scaling)\n",
    "#    - Standardized (Z-score Normalization)\n",
    "# 3. Define the softmax function and cross-entropy loss.\n",
    "# 4. Implement training with both gradient descent (GD) and stochastic gradient descent (SGD).\n",
    "# 5. Evaluate and visualize the training performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_iris\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 2. Load the Iris Dataset\n",
    "# \n",
    "# We load the dataset using `sklearn.datasets.load_iris()`, extract the four features and class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_iris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the Iris dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m iris = \u001b[43mload_iris\u001b[49m()\n\u001b[32m      3\u001b[39m X = iris.data    \u001b[38;5;66;03m# shape: (150, 4)\u001b[39;00m\n\u001b[32m      4\u001b[39m y = iris.target  \u001b[38;5;66;03m# shape: (150,)\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'load_iris' is not defined"
     ]
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data    # (150, 4)\n",
    "y = iris.target  # (150,)\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Labels shape:\", y.shape)\n",
    "print(\"Unique classes:\", np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 3. Preprocess the Data\n",
    "# \n",
    "# We first split the data into training (60%) and testing (40%) sets. Then we define two preprocessing functions:\n",
    "# \n",
    "# - **Normalization (Min-Max Scaling):** Rescales features to the range [0, 1].  \n",
    "# - **Standardization (Z-score):** Transforms features to have mean 0 and standard deviation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Split the dataset (60% training, 40% testing)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m(X, y, test_size=\u001b[32m0.4\u001b[39m, random_state=\u001b[32m42\u001b[39m, stratify=y)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormalize\u001b[39m(X):\n\u001b[32m      5\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Min-Max Normalization to [0,1]\"\"\"\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the dataset (60% training, 40% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "def normalize(X):\n",
    "    \"\"\"Min-Max Normalization to [0,1]\"\"\"\n",
    "    return (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\"Z-score Standardization\"\"\"\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Preprocess data\n",
    "X_train_norm = normalize(X_train)\n",
    "X_test_norm  = normalize(X_test)\n",
    "\n",
    "X_train_std = standardize(X_train)\n",
    "X_test_std  = standardize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Helper Functions\n",
    "# \n",
    "# We now define the following:\n",
    "# \n",
    "# - **Softmax function:** Computes probabilities from logits.\n",
    "# - **One-hot encoding:** Converts integer class labels into one-hot vectors.\n",
    "# - **Categorical cross-entropy loss:** Computes the loss over m samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Compute the softmax of each row of the input z.\n",
    "    \"\"\"\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    \"\"\"\n",
    "    Convert label vector y to one-hot encoded matrix.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    y_encoded = np.zeros((m, num_classes))\n",
    "    y_encoded[np.arange(m), y] = 1\n",
    "    return y_encoded\n",
    "\n",
    "def compute_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the categorical cross-entropy loss.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    loss = -np.sum(y_true * np.log(y_pred + 1e-15)) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 5. Implement the Multiclass Logistic Regression Model\n",
    "# \n",
    "# We create a class `LogisticRegressionMulti` that implements:\n",
    "# \n",
    "# - **Batch Gradient Descent (GD):** The weight update is performed using all samples.\n",
    "# - **Stochastic Gradient Descent (SGD):** The weight update is performed on one sample at a time.\n",
    "# \n",
    "# Both methods track the training loss at every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionMulti:\n",
    "    def __init__(self, learning_rate=0.01, epochs=1000, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit_gd(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Batch Gradient Descent.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.W = np.zeros((n, num_classes))\n",
    "        y_encoded = one_hot_encode(y, num_classes)\n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            logits = np.dot(X, self.W)\n",
    "            probs = softmax(logits)\n",
    "            loss = compute_loss(y_encoded, probs)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            # Gradient calculation\n",
    "            grad = np.dot(X.T, (probs - y_encoded)) / m\n",
    "            self.W -= self.learning_rate * grad\n",
    "            \n",
    "            if self.verbose and epoch % 100 == 0:\n",
    "                print(f\"[GD] Epoch {epoch}/{self.epochs} - Loss: {loss:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def fit_sgd(self, X, y):\n",
    "        \"\"\"\n",
    "        Train using Stochastic Gradient Descent.\n",
    "        \"\"\"\n",
    "        m, n = X.shape\n",
    "        num_classes = len(np.unique(y))\n",
    "        self.W = np.zeros((n, num_classes))\n",
    "        y_encoded = one_hot_encode(y, num_classes)\n",
    "        self.loss_history = []\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.arange(m)\n",
    "            np.random.shuffle(indices)\n",
    "            for i in indices:\n",
    "                xi = X[i:i+1]  # shape (1, n)\n",
    "                yi = y_encoded[i:i+1]  # shape (1, num_classes)\n",
    "                logits = np.dot(xi, self.W)\n",
    "                probs = softmax(logits)\n",
    "                grad = np.dot(xi.T, (probs - yi))\n",
    "                self.W -= self.learning_rate * grad\n",
    "            # Compute loss at the end of epoch\n",
    "            logits = np.dot(X, self.W)\n",
    "            probs = softmax(logits)\n",
    "            loss = compute_loss(y_encoded, probs)\n",
    "            self.loss_history.append(loss)\n",
    "            \n",
    "            if self.verbose and epoch % 100 == 0:\n",
    "                print(f\"[SGD] Epoch {epoch}/{self.epochs} - Loss: {loss:.4f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logits = np.dot(X, self.W)\n",
    "        probs = softmax(logits)\n",
    "        return np.argmax(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## 6. Train and Evaluate the Model\n",
    "# \n",
    "# We now run experiments using both GD and SGD. In each case, we demonstrate training on (for example) normalized data.\n",
    "# \n",
    "# You can repeat the following steps with:\n",
    "# - The unprocessed data: `X_train, X_test`\n",
    "# - The standardized data: `X_train_std, X_test_std`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### 6.1 Training with Batch Gradient Descent (GD) on Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create and train the model using GD on normalized data\u001b[39;00m\n\u001b[32m      2\u001b[39m model_gd = LogisticRegressionMulti(learning_rate=\u001b[32m0.1\u001b[39m, epochs=\u001b[32m1000\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model_gd.fit_gd(\u001b[43mX_train_norm\u001b[49m, y_train)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Make predictions on test set\u001b[39;00m\n\u001b[32m      6\u001b[39m y_pred_gd = model_gd.predict(X_test_norm)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Train using Batch Gradient Descent on normalized data\n",
    "model_gd = LogisticRegressionMulti(learning_rate=0.1, epochs=1000, verbose=True)\n",
    "model_gd.fit_gd(X_train_norm, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_gd = model_gd.predict(X_test_norm)\n",
    "accuracy_gd = accuracy_score(y_test, y_pred_gd)\n",
    "print(\"\\n[GD] Test Accuracy: {:.2f}%\".format(accuracy_gd * 100))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_gd))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### 6.2 Visualize the Loss Curve for GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      2\u001b[39m plt.plot(model_gd.loss_history, label=\u001b[33m\"\u001b[39m\u001b[33mGD Loss\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpochs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model_gd.loss_history, label=\"GD Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss Curve (Gradient Descent)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 6.3 Training with Stochastic Gradient Descent (SGD) on Normalized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create and train the model using SGD on normalized data\u001b[39;00m\n\u001b[32m      2\u001b[39m model_sgd = LogisticRegressionMulti(learning_rate=\u001b[32m0.01\u001b[39m, epochs=\u001b[32m1000\u001b[39m, verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model_sgd.fit_sgd(\u001b[43mX_train_norm\u001b[49m, y_train)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Make predictions on test set\u001b[39;00m\n\u001b[32m      6\u001b[39m y_pred_sgd = model_sgd.predict(X_test_norm)\n",
      "\u001b[31mNameError\u001b[39m: name 'X_train_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Train using Stochastic Gradient Descent on normalized data\n",
    "model_sgd = LogisticRegressionMulti(learning_rate=0.01, epochs=1000, verbose=True)\n",
    "model_sgd.fit_sgd(X_train_norm, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_sgd = model_sgd.predict(X_test_norm)\n",
    "accuracy_sgd = accuracy_score(y_test, y_pred_sgd)\n",
    "print(\"\\n[SGD] Test Accuracy: {:.2f}%\".format(accuracy_sgd * 100))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_sgd))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_sgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ### 6.4 Visualize the Loss Curve for SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m      2\u001b[39m plt.plot(model_sgd.loss_history, label=\u001b[33m\"\u001b[39m\u001b[33mSGD Loss\u001b[39m\u001b[33m\"\u001b[39m, color=\u001b[33m'\u001b[39m\u001b[33morange\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mEpochs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(model_sgd.loss_history, label=\"SGD Loss\", color='orange')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Cross-Entropy Loss\")\n",
    "plt.title(\"Training Loss Curve (Stochastic Gradient Descent)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
